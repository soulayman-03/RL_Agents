# RL-Based DNN Layer Scheduling on IoT Devices

This repository contains experiments for **reinforcement learning (RL)** to schedule the execution of **DNN layers** across a set of heterogeneous **IoT devices**, under multiple system constraints (privacy, bandwidth, compute, memory, energy, trust, etc.).

The core idea is:
- A **task/model** is represented as a sequence of **layers** (compute/memory/output/privacy requirements).
- A set of **devices** provide CPU speed, memory capacity, bandwidth, and privacy clearance.
- At each step, an RL agent chooses **which device executes the next layer**.
- The environment returns a reward that is mainly the **negative latency**, with penalties/terminations when constraints are violated.

## Core Building Blocks

### Model / Layer Profiles
Model “architectures” in this project are **profiles** of layer requirements (not full neural nets), generated by:
- `SingleAgent/utils.py` → `generate_specific_model(model_type)`

Each layer has:
- `computation_demand`, `memory_demand`, `output_data_size`, `privacy_level`

Supported profiles include (non-exhaustive):
- `lenet`, `simplecnn`, `deepcnn`, `miniresnet`
- synthetic depth variants: `cnn7`, `cnn10`, `cnn15`
- larger profiles: `bigcnn`, `biggercnn`, `hugecnn` (and alias `hugcnn`)

### Devices / Resource Manager
Devices are generated with heterogeneous characteristics (CPU/RAM/BW/privacy clearance) and managed by:
- `integrated_system/resource_manager.py` (`ResourceManager`)

The ResourceManager is responsible for enforcing multiple “hard” allocation constraints and reporting failures using `fail.reason` values such as:
- `sequential_diversity`
- `privacy`
- `memory`
- `compute`
- `bandwidth`
- `privacy_exposure`
- `security_level_exposure` (S_l / max exposure fraction)

### Environments (Gymnasium)
The environments wrap the scheduling logic and compute rewards.

Main ones you will encounter:
- `MultiAgent/environment.py` → `MultiAgentIoTEnv` (baseline multi-agent scheduling)
- `MultiAgentMADDPG_Energy/environment.py` → `MultiAgentIoTEnvEnergyHard` (adds **hard energy budget**)
- `MultiAgentMADDPG_PrivacyWeighted/environment.py` → trust-based privacy constraint (**soft or hard**)
- `MultiAgentMADDPG_AllConstraints/environment.py` → a unified environment combining **ResourceManager + energy + trust**

## Constraints Implemented

### 1) Resource Constraints (ResourceManager)
Always applied in environments that call `ResourceManager.can_allocate(...) / try_allocate(...)`:
- **Sequential diversity**: discourages placing consecutive layers on the same device.
- **Privacy clearance**: device clearance must be sufficient for the layer (hard check).
- **Memory capacity**: current usage + demand must fit.
- **Compute capacity**: per-step compute must fit device limits.
- **Bandwidth capacity**: per-step transmission must fit device limits.
- **Privacy exposure**: prevents exposing the entire model to a single device in certain cases.
- **S_l / security-level exposure**: limits how many layers of an agent can be assigned to one device using `max_exposure_fraction`.
- **Queueing (optional)**: `queue_per_device` simulates FIFO waiting time when multiple agents select the same device.

### 2) Energy (Hard Budget per Device)
Implemented in:
- `MultiAgentMADDPG_Energy/environment.py` (EnergyHard variant)
- `MultiAgentMADDPG_AllConstraints/environment.py` (unified variant)

Behavior:
- Each device has an **episode energy budget**.
- Each allocation consumes energy:
  - `energy_cost = alpha_comp * computation_demand + alpha_comm * transmitted_data`
- If remaining energy is insufficient:
  - allocation fails with `reason="energy"`, reward `-500`, and the agent terminates.

### 3) Trust / Minimal Trust Threshold (Hard or Soft)
Implemented in:
- `MultiAgentMADDPG_PrivacyWeighted/environment.py`
- `MultiAgentMADDPG_AllConstraints/environment.py`

Behavior:
- Each device has a `trust_score ∈ [0,1]` (in current setup often constrained to `[0.5, 1)`).
- Each layer has `privacy_level ∈ [0..privacy_max_level]`.
- A required trust is derived from privacy:
  - `trust_required = trust_min_for_max_privacy * (privacy_level / privacy_max_level)`
- **Hard trust** (`trust_hard=True`): if `trust_score < trust_required` → fail `reason="trust"`, reward `-500`.
- **Soft trust** (`trust_hard=False`): allow allocation but add penalty:
  - `trust_penalty = trust_lambda * max(0, trust_required - trust_score)`

## RL Algorithms / Folders

### Single Agent
- `SingleAgent/`
  - DQN: `SingleAgent/dqn_train.py`
  - SARSA: `SingleAgent/sarsa_train.py`
  - Actor-Critic (A2C-style): `SingleAgent/ac_train.py`

### Multi Agent (MARL)
- `MultiAgent/` (baseline env + scripts)
- `MultiAgentMADDPG/` (MADDPG with discrete actors + centralized critic)
- `MultiAgentMATD3/` (MATD3 experiments)
- `MultiAgentVDN/` (VDN)
- `MultiAgentQMIX/` (QMIX)
- `MultiAgentMADDPG_Energy/` (MADDPG + EnergyHard environment)
- `MultiAgentMADDPG_PrivacyWeighted/` (MADDPG + trust-based privacy constraint)
- `MultiAgentMADDPG_AllConstraints/` (MADDPG + **all constraints in one env**)

## Results / Artifacts Produced

Most training scripts write a run folder that contains:
- `run_config.json`: run parameters (algo, seed, S_l, trust/energy knobs, etc.)
- `model_summary.json`: model types + per-layer specs (compute/memory/output/privacy)
- `device_summary.json`: per-device specs (CPU/RAM/BW/privacy + trust/energy if applicable)
- `summary.json`: aggregated metrics at the end of training
- `train_log.jsonl`: one JSON line per episode (rewards, success/fail, trace, etc.)
- `plots/`: PNG plots such as:
  - `training_trends.png` (team return mean/step + loss)
  - `avg_cumulative_rewards.png` (sum over episode, normalized per agent)
  - `training_agent_rewards.png`, `training_agent_success_rate.png`
  - `policy_value_losses.png` (actor vs critic)
  - `layer_latency_breakdown.png` (t_comp vs t_comm by layer)
  - `execution_strategy.png` / `execution_flow.png` (layer→device strategy)

## Quick Run Examples

### Trust-based privacy (hard trust by default)
`python -m MultiAgentMADDPG_PrivacyWeighted.train --episodes 200 --log-every 10 --trust-min-for-max-privacy 0.8`

Switch to soft trust penalty:
`python -m MultiAgentMADDPG_PrivacyWeighted.train --episodes 200 --log-every 10 --trust-soft --trust-min-for-max-privacy 0.8`

### EnergyHard (separate energy environment)
`python -m MultiAgentMADDPG_Energy.train --episodes 200 --log-every 10 --energy-min 500 --energy-max 1200 --alpha-comp 1 --alpha-comm 1`

### All constraints in one environment
`python -m MultiAgentMADDPG_AllConstraints.train --episodes 200 --log-every 10 --trust-min-for-max-privacy 0.8 --energy-min 500 --energy-max 1200`

